{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('wordpress_indexing_requester.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Rate limits\n",
    "CALLS_PER_MINUTE = 60\n",
    "ONE_MINUTE = 60\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS_PER_MINUTE, period=ONE_MINUTE)\n",
    "def rate_limited_api_call(func):\n",
    "    return func()\n",
    "\n",
    "def retry_on_error(func):\n",
    "    \"\"\"Decorator to retry API calls on error with exponential backoff.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except HttpError as e:\n",
    "                if e.resp.status == 500:  # Internal server error\n",
    "                    retry_count += 1\n",
    "                    if retry_count == max_retries:\n",
    "                        logger.error(f\"Max retries reached for API call: {e}\")\n",
    "                        return None\n",
    "                    wait_time = 2 ** retry_count  # Exponential backoff\n",
    "                    logger.info(f\"API call failed with 500 error, retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                raise\n",
    "    return wrapper\n",
    "\n",
    "class WordPressIndexRequester:\n",
    "    def __init__(self, gsc_credentials_file: str, domains: List[str], batch_size: int = 5):\n",
    "        \"\"\"Initialize the WordPress Index Requester.\"\"\"\n",
    "        self.domains = [self._format_domain_url(domain) for domain in domains]\n",
    "        self.batch_size = batch_size\n",
    "        self.credentials = self._load_credentials(gsc_credentials_file)\n",
    "        self.service = build(\"searchconsole\", \"v1\", credentials=self.credentials)\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        self.indexed_cache = {}  # Cache for indexing status\n",
    "\n",
    "    def _load_credentials(self, credentials_file: str) -> Credentials:\n",
    "        \"\"\"Load Google Search Console credentials.\"\"\"\n",
    "        try:\n",
    "            return Credentials.from_service_account_file(\n",
    "                credentials_file,\n",
    "                scopes=[\"https://www.googleapis.com/auth/webmasters\"]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load credentials: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _format_domain_url(self, domain: str) -> str:\n",
    "        \"\"\"Format domain URL with proper scheme.\"\"\"\n",
    "        domain = domain.strip().lower()\n",
    "        if not domain.startswith(('http://', 'https://')):\n",
    "            domain = 'https://' + domain\n",
    "        return domain.rstrip('/')\n",
    "\n",
    "    def _get_site_url(self, url: str) -> str:\n",
    "        \"\"\"Get site URL from any URL.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}/\"\n",
    "\n",
    "    @retry_on_error\n",
    "    def check_google_indexing(self, url: str) -> Optional[bool]:\n",
    "        \"\"\"Check if a URL is indexed in Google Search with retry logic.\"\"\"\n",
    "        # Check cache first\n",
    "        if url in self.indexed_cache:\n",
    "            return self.indexed_cache[url]\n",
    "\n",
    "        try:\n",
    "            url = self._format_domain_url(url)\n",
    "            site_url = self._get_site_url(url)\n",
    "            \n",
    "            request = self.service.urlInspection().index().inspect(\n",
    "                body={\n",
    "                    \"inspectionUrl\": url,\n",
    "                    \"siteUrl\": site_url\n",
    "                }\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            inspection_result = response.get(\"inspectionResult\", {})\n",
    "            index_status = inspection_result.get(\"indexStatusResult\", {})\n",
    "            coverage_state = index_status.get(\"coverageState\")\n",
    "            \n",
    "            is_indexed = coverage_state == \"INDEXED\"\n",
    "            self.indexed_cache[url] = is_indexed  # Cache the result\n",
    "            return is_indexed\n",
    "\n",
    "        except HttpError as e:\n",
    "            if e.resp.status == 429:  # Rate limit exceeded\n",
    "                logger.warning(\"Rate limit exceeded, waiting before retry...\")\n",
    "                time.sleep(60)  # Wait for 60 seconds\n",
    "                return self.check_google_indexing(url)  # Retry\n",
    "            else:\n",
    "                logger.error(f\"Error checking indexing status for {url}: {e}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking indexing status for {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_non_indexed_posts(self, domain: str, max_retries: int = 3) -> List[Dict]:\n",
    "        \"\"\"Get all published posts from a WordPress domain and check their indexing status.\"\"\"\n",
    "        posts = []\n",
    "        page = 1\n",
    "        retries = max_retries\n",
    "        base_url = domain\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                endpoint = f\"{base_url}/wp-json/wp/v2/posts\"\n",
    "                response = self.session.get(\n",
    "                    endpoint,\n",
    "                    params={\n",
    "                        'page': page,\n",
    "                        'per_page': 100,\n",
    "                        'status': 'publish'\n",
    "                    },\n",
    "                    timeout=15\n",
    "                )\n",
    "                \n",
    "                # Handle pagination end gracefully\n",
    "                if response.status_code == 400:\n",
    "                    logger.info(f\"Reached end of posts at page {page} for {base_url}\")\n",
    "                    break\n",
    "                \n",
    "                response.raise_for_status()\n",
    "                current_posts = response.json()\n",
    "                \n",
    "                if not current_posts:\n",
    "                    break\n",
    "\n",
    "                # Check indexing status for each post\n",
    "                for post in current_posts:\n",
    "                    post_url = post.get(\"link\")\n",
    "                    if post_url:\n",
    "                        def check_index():\n",
    "                            return self.check_google_indexing(post_url)\n",
    "                        \n",
    "                        indexed = rate_limited_api_call(check_index)\n",
    "                        if indexed is False:  # Only add non-indexed posts\n",
    "                            posts.append(post)\n",
    "\n",
    "                page += 1\n",
    "                retries = max_retries\n",
    "                time.sleep(1)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"Error fetching posts from {base_url}: {e}\")\n",
    "                retries -= 1\n",
    "                if retries <= 0:\n",
    "                    break\n",
    "                time.sleep(2 ** (max_retries - retries))\n",
    "\n",
    "        return posts\n",
    "\n",
    "    @retry_on_error\n",
    "    def request_indexing(self, url: str) -> Dict:\n",
    "        \"\"\"Request indexing for a specific URL using the Indexing API.\"\"\"\n",
    "        try:\n",
    "            url = self._format_domain_url(url)\n",
    "            site_url = self._get_site_url(url)\n",
    "            \n",
    "            request = self.service.urlInspection().index().inspect(\n",
    "                body={\n",
    "                    \"inspectionUrl\": url,\n",
    "                    \"siteUrl\": site_url\n",
    "                }\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            return {\n",
    "                \"URL\": url,\n",
    "                \"Status\": \"Success\",\n",
    "                \"Message\": \"Indexing requested successfully\",\n",
    "                \"Response\": str(response),\n",
    "                \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error requesting indexing for {url}: {e}\")\n",
    "            return {\n",
    "                \"URL\": url,\n",
    "                \"Status\": \"Failed\",\n",
    "                \"Message\": str(e),\n",
    "                \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "\n",
    "    def process_domain(self, domain: str) -> List[Dict]:\n",
    "        \"\"\"Process a single domain and request indexing for non-indexed posts.\"\"\"\n",
    "        results = []\n",
    "        logger.info(f\"Processing domain: {domain}\")\n",
    "        \n",
    "        non_indexed_posts = self.get_non_indexed_posts(domain)\n",
    "        logger.info(f\"Found {len(non_indexed_posts)} non-indexed posts for {domain}\")\n",
    "        \n",
    "        for post in non_indexed_posts:\n",
    "            post_url = post.get(\"link\")\n",
    "            if post_url:\n",
    "                result = self.request_indexing(post_url)\n",
    "                results.append(result)\n",
    "                time.sleep(1)  # Rate limiting between requests\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_all_domains(self) -> pd.DataFrame:\n",
    "        \"\"\"Process all domains and request indexing for non-indexed posts.\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        for i in range(0, len(self.domains), self.batch_size):\n",
    "            batch = self.domains[i:i + self.batch_size]\n",
    "            current_batch = i // self.batch_size + 1\n",
    "            total_batches = (len(self.domains) + self.batch_size - 1) // self.batch_size\n",
    "            logger.info(f\"Processing batch {current_batch} of {total_batches}\")\n",
    "            \n",
    "            for domain in batch:\n",
    "                try:\n",
    "                    results = self.process_domain(domain)\n",
    "                    all_results.extend(results)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing domain {domain}: {e}\")\n",
    "                \n",
    "                time.sleep(2)  # Delay between domains\n",
    "            \n",
    "            if current_batch < total_batches:\n",
    "                logger.info(\"Waiting between batches...\")\n",
    "                time.sleep(10)  # Delay between batches\n",
    "        \n",
    "        return pd.DataFrame(all_results)\n",
    "\n",
    "def main():\n",
    "    GSC_CREDENTIALS_FILE = \"path/to/your/credentials.json\"  # Placeholder for credentials file path\n",
    "    DOMAINS = [\n",
    "        \"example.com\"  # Placeholder for domains Add your domain names here\n",
    "    ]\n",
    "    try:\n",
    "        requester = WordPressIndexRequester(GSC_CREDENTIALS_FILE, DOMAINS, batch_size=5)\n",
    "        results_df = requester.process_all_domains()\n",
    "        \n",
    "        if not results_df.empty:\n",
    "            output_file = f\"indexing_requests_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "            results_df.to_csv(output_file, index=False)\n",
    "            logger.info(f\"Report generated successfully: {output_file}\")\n",
    "        else:\n",
    "            logger.warning(\"No URLs were processed for indexing requests.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script execution failed: {e}\")\n",
    "        raise\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
